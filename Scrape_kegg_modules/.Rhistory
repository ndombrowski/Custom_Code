KO_char = re.sub(r'\((.*?)\)',r'\1', KO_char)
KO_char
KO_char = KOs[:]
KO_char = KO_char.strip('\n ')
KO_char = KO_char.partition('\n')[0]
#KO_char = re.sub(r'(K[0-9]+)K',r'\1 K', KO_char)
KO_char = re.sub(r'(K[0-9]+(?=K))',r'\1 ', KO_char)
KO_char = re.sub("\s+", ";", KO_char.strip())
KO_char = re.sub(r"\([^\(\)]+\)", lambda x: x.group(0).replace(";", ","), KO_char)
KO_char = re.sub(r'\((.*?)\)',r'\1', KO_char)
KO_char = re.sub(r"\([^\(\)]+\)", lambda x: x.group(0).replace(";", ","), KO_char)
KO_char = re.sub(r'\((.*?)\)',r'\1', KO_char)
KO_char = re.sub("\(", "", KO_char.strip())
KO_char = re.sub("\)", "", KO_char.strip())
KO_char = re.sub("\+", ",", KO_char.strip())
KO_char = re.sub("\-", ",", KO_char.strip())
KO_char = re.sub("-", '', KO_char)
#KO_char = KO_char.split(";")
#print to df
df = pd.DataFrame({'KO_ID': KO_char})
#add dummy columns for the order and add column for module ID
df["Order"] = np.arange(df.shape[0])+1
df["Module"] = module_id
#split
df['KO_ID'] = df['KO_ID'].str.split(',')
df = df.explode('KO_ID')
df
#print
#df.to_csv(module_id+'.txt', sep ='\t', index = False)
KO_char = KOs[:]
KO_char = KO_char.strip('\n ')
KO_char = KO_char.partition('\n')[0]
#KO_char = re.sub(r'(K[0-9]+)K',r'\1 K', KO_char)
KO_char = re.sub(r'(K[0-9]+(?=K))',r'\1 ', KO_char)
KO_char = re.sub("\s+", ";", KO_char.strip())
KO_char = re.sub(r"\([^\(\)]+\)", lambda x: x.group(0).replace(";", ","), KO_char)
KO_char = re.sub(r'\((.*?)\)',r'\1', KO_char)
KO_char = re.sub(r"\([^\(\)]+\)", lambda x: x.group(0).replace(";", ","), KO_char)
KO_char = re.sub(r'\((.*?)\)',r'\1', KO_char)
KO_char = re.sub("\(", "", KO_char.strip())
KO_char = re.sub("\)", "", KO_char.strip())
KO_char = re.sub("\+", ",", KO_char.strip())
KO_char = re.sub("\-", ",", KO_char.strip())
KO_char = re.sub("-", '', KO_char)
KO_char = KO_char.split(";")
#print to df
df = pd.DataFrame({'KO_ID': KO_char})
#add dummy columns for the order and add column for module ID
df["Order"] = np.arange(df.shape[0])+1
df["Module"] = module_id
#split
df['KO_ID'] = df['KO_ID'].str.split(',')
df = df.explode('KO_ID')
df
#print
#df.to_csv(module_id+'.txt', sep ='\t', index = False)
module_id = "M00001"
url = "https://www.genome.jp/module/" + module_id
#use get to download the contents of the webpage in text format and store in a variable called data:
data = requests.get(url).text
#create a beautiful soup object
soup = BeautifulSoup(data,"html.parser")
Definitions = soup.findAll("div", class_ = 'definition')
for result in Definitions:
categories = result.find_all("td")
KOs = categories[5].text
print()
#print to df
df = pd.DataFrame({'KO_ID': KO_char})
#add dummy columns for the order and add column for module ID
df["Order"] = np.arange(df.shape[0])+1
df["Module"] = module_id
#split
df['KO_ID'] = df['KO_ID'].str.split(',')
df = df.explode('KO_ID')
df
#print
#df.to_csv(module_id+'.txt', sep ='\t', index = False)
df
module_id = "M00001"
url = "https://www.genome.jp/module/" + module_id
#use get to download the contents of the webpage in text format and store in a variable called data:
data = requests.get(url).text
#create a beautiful soup object
soup = BeautifulSoup(data,"html.parser")
Definitions = soup.findAll("div", class_ = 'definition')
for result in Definitions:
categories = result.find_all("td")
KOs = categories[5].text
print()
KO_char = KOs[:]
KO_char = KO_char.strip('\n ')
KO_char = KO_char.partition('\n')[0]
#KO_char = re.sub(r'(K[0-9]+)K',r'\1 K', KO_char)
KO_char = re.sub(r'(K[0-9]+(?=K))',r'\1 ', KO_char)
KO_char = re.sub("\s+", ";", KO_char.strip())
KO_char = re.sub(r"\([^\(\)]+\)", lambda x: x.group(0).replace(";", ","), KO_char)
KO_char = re.sub(r'\((.*?)\)',r'\1', KO_char)
KO_char = re.sub(r"\([^\(\)]+\)", lambda x: x.group(0).replace(";", ","), KO_char)
KO_char = re.sub(r'\((.*?)\)',r'\1', KO_char)
KO_char = re.sub("\(", "", KO_char.strip())
KO_char = re.sub("\)", "", KO_char.strip())
KO_char = re.sub("\+", ",", KO_char.strip())
KO_char = re.sub("\-", ",", KO_char.strip())
KO_char = re.sub("-", '', KO_char)
KO_char = KO_char.split(";")
#print to df
df = pd.DataFrame({'KO_ID': KO_char})
#add dummy columns for the order and add column for module ID
df["Order"] = np.arange(df.shape[0])+1
df["Module"] = module_id
#split
df['KO_ID'] = df['KO_ID'].str.split(',')
df = df.explode('KO_ID')
df
#print
#df.to_csv(module_id+'.txt', sep ='\t', index = False)
module_id = "M00003"
url = "https://www.genome.jp/module/" + module_id
#use get to download the contents of the webpage in text format and store in a variable called data:
data = requests.get(url).text
#create a beautiful soup object
soup = BeautifulSoup(data,"html.parser")
Definitions = soup.findAll("div", class_ = 'definition')
for result in Definitions:
categories = result.find_all("td")
KOs = categories[5].text
print()
KO_char = KOs[:]
KO_char = KO_char.strip('\n ')
KO_char = KO_char.partition('\n')[0]
#KO_char = re.sub(r'(K[0-9]+)K',r'\1 K', KO_char)
KO_char = re.sub(r'(K[0-9]+(?=K))',r'\1 ', KO_char)
KO_char = re.sub("\s+", ";", KO_char.strip())
KO_char = re.sub(r"\([^\(\)]+\)", lambda x: x.group(0).replace(";", ","), KO_char)
KO_char = re.sub(r'\((.*?)\)',r'\1', KO_char)
KO_char = re.sub(r"\([^\(\)]+\)", lambda x: x.group(0).replace(";", ","), KO_char)
KO_char = re.sub(r'\((.*?)\)',r'\1', KO_char)
KO_char = re.sub("\(", "", KO_char.strip())
KO_char = re.sub("\)", "", KO_char.strip())
KO_char = re.sub("\+", ",", KO_char.strip())
KO_char = re.sub("\-", ",", KO_char.strip())
KO_char = re.sub("-", '', KO_char)
KO_char = KO_char.split(";")
#print to df
df = pd.DataFrame({'KO_ID': KO_char})
#add dummy columns for the order and add column for module ID
df["Order"] = np.arange(df.shape[0])+1
df["Module"] = module_id
#split
df['KO_ID'] = df['KO_ID'].str.split(',')
df = df.explode('KO_ID')
df
#print
#df.to_csv(module_id+'.txt', sep ='\t', index = False)
module_id = "M00307"
url = "https://www.genome.jp/module/" + module_id
#use get to download the contents of the webpage in text format and store in a variable called data:
data = requests.get(url).text
#create a beautiful soup object
soup = BeautifulSoup(data,"html.parser")
Definitions = soup.findAll("div", class_ = 'definition')
for result in Definitions:
categories = result.find_all("td")
KOs = categories[5].text
print()
KO_char = KOs[:]
KO_char = KO_char.strip('\n ')
KO_char = KO_char.partition('\n')[0]
#KO_char = re.sub(r'(K[0-9]+)K',r'\1 K', KO_char)
KO_char = re.sub(r'(K[0-9]+(?=K))',r'\1 ', KO_char)
KO_char = re.sub("\s+", ";", KO_char.strip())
KO_char = re.sub(r"\([^\(\)]+\)", lambda x: x.group(0).replace(";", ","), KO_char)
KO_char = re.sub(r'\((.*?)\)',r'\1', KO_char)
KO_char = re.sub(r"\([^\(\)]+\)", lambda x: x.group(0).replace(";", ","), KO_char)
KO_char = re.sub(r'\((.*?)\)',r'\1', KO_char)
KO_char = re.sub("\(", "", KO_char.strip())
KO_char = re.sub("\)", "", KO_char.strip())
KO_char = re.sub("\+", ",", KO_char.strip())
KO_char = re.sub("\-", ",", KO_char.strip())
KO_char = re.sub("-", '', KO_char)
KO_char = KO_char.split(";")
#print to df
df = pd.DataFrame({'KO_ID': KO_char})
#add dummy columns for the order and add column for module ID
df["Order"] = np.arange(df.shape[0])+1
df["Module"] = module_id
#split
df['KO_ID'] = df['KO_ID'].str.split(',')
df = df.explode('KO_ID')
df
#print
#df.to_csv(module_id+'.txt', sep ='\t', index = False)
module_id = "M00031"
url = "https://www.genome.jp/module/" + module_id
#use get to download the contents of the webpage in text format and store in a variable called data:
data = requests.get(url).text
#create a beautiful soup object
soup = BeautifulSoup(data,"html.parser")
Definitions = soup.findAll("div", class_ = 'definition')
for result in Definitions:
categories = result.find_all("td")
KOs = categories[5].text
print()
KO_char = KOs[:]
KO_char = KO_char.strip('\n ')
KO_char = KO_char.partition('\n')[0]
#KO_char = re.sub(r'(K[0-9]+)K',r'\1 K', KO_char)
KO_char = re.sub(r'(K[0-9]+(?=K))',r'\1 ', KO_char)
KO_char = re.sub("\s+", ";", KO_char.strip())
KO_char = re.sub(r"\([^\(\)]+\)", lambda x: x.group(0).replace(";", ","), KO_char)
KO_char = re.sub(r'\((.*?)\)',r'\1', KO_char)
KO_char = re.sub(r"\([^\(\)]+\)", lambda x: x.group(0).replace(";", ","), KO_char)
KO_char = re.sub(r'\((.*?)\)',r'\1', KO_char)
KO_char = re.sub("\(", "", KO_char.strip())
KO_char = re.sub("\)", "", KO_char.strip())
KO_char = re.sub("\+", ",", KO_char.strip())
KO_char = re.sub("\-", ",", KO_char.strip())
KO_char = re.sub("-", '', KO_char)
KO_char = KO_char.split(";")
#print to df
df = pd.DataFrame({'KO_ID': KO_char})
#add dummy columns for the order and add column for module ID
df["Order"] = np.arange(df.shape[0])+1
df["Module"] = module_id
#split
df['KO_ID'] = df['KO_ID'].str.split(',')
df = df.explode('KO_ID')
df
#print
#df.to_csv(module_id+'.txt', sep ='\t', index = False)
module_id = "M00135"
url = "https://www.genome.jp/module/" + module_id
#use get to download the contents of the webpage in text format and store in a variable called data:
data = requests.get(url).text
#create a beautiful soup object
soup = BeautifulSoup(data,"html.parser")
Definitions = soup.findAll("div", class_ = 'definition')
for result in Definitions:
categories = result.find_all("td")
KOs = categories[5].text
print()
KO_char = KOs[:]
KO_char = KO_char.strip('\n ')
KO_char = KO_char.partition('\n')[0]
#KO_char = re.sub(r'(K[0-9]+)K',r'\1 K', KO_char)
KO_char = re.sub(r'(K[0-9]+(?=K))',r'\1 ', KO_char)
KO_char = re.sub("\s+", ";", KO_char.strip())
KO_char = re.sub(r"\([^\(\)]+\)", lambda x: x.group(0).replace(";", ","), KO_char)
KO_char = re.sub(r'\((.*?)\)',r'\1', KO_char)
KO_char = re.sub(r"\([^\(\)]+\)", lambda x: x.group(0).replace(";", ","), KO_char)
KO_char = re.sub(r'\((.*?)\)',r'\1', KO_char)
KO_char = re.sub("\(", "", KO_char.strip())
KO_char = re.sub("\)", "", KO_char.strip())
KO_char = re.sub("\+", ",", KO_char.strip())
KO_char = re.sub("\-", ",", KO_char.strip())
KO_char = re.sub("-", '', KO_char)
KO_char = KO_char.split(";")
#print to df
df = pd.DataFrame({'KO_ID': KO_char})
#add dummy columns for the order and add column for module ID
df["Order"] = np.arange(df.shape[0])+1
df["Module"] = module_id
#split
df['KO_ID'] = df['KO_ID'].str.split(',')
df = df.explode('KO_ID')
df
#print
#df.to_csv(module_id+'.txt', sep ='\t', index = False)
df[df.Name != '']
df
df[df.KO_ID != '']
#print to df
df = pd.DataFrame({'KO_ID': KO_char})
#add dummy columns for the order and add column for module ID
df["Order"] = np.arange(df.shape[0])+1
df["Module"] = module_id
#split
df['KO_ID'] = df['KO_ID'].str.split(',')
df = df.explode('KO_ID')
#drop empty columns
df= df[df.KO_ID != '']
df
#print
#df.to_csv(module_id+'.txt', sep ='\t', index = False)
module_id = "M00014"
url = "https://www.genome.jp/module/" + module_id
#use get to download the contents of the webpage in text format and store in a variable called data:
data = requests.get(url).text
#create a beautiful soup object
soup = BeautifulSoup(data,"html.parser")
module_id = "M00014"
url = "https://www.genome.jp/module/" + module_id
#use get to download the contents of the webpage in text format and store in a variable called data:
data = requests.get(url).text
#create a beautiful soup object
soup = BeautifulSoup(data,"html.parser")
Definitions = soup.findAll("div", class_ = 'definition')
for result in Definitions:
categories = result.find_all("td")
KOs = categories[5].text
print()
#print to df
df = pd.DataFrame({'KO_ID': KO_char})
#add dummy columns for the order and add column for module ID
df["Order"] = np.arange(df.shape[0])+1
df["Module"] = module_id
#split
df['KO_ID'] = df['KO_ID'].str.split(',')
df = df.explode('KO_ID')
#drop empty columns
df= df[df.KO_ID != '']
df
#print
#df.to_csv(module_id+'.txt', sep ='\t', index = False)
module_id = "M00014"
url = "https://www.genome.jp/module/" + module_id
#use get to download the contents of the webpage in text format and store in a variable called data:
data = requests.get(url).text
#create a beautiful soup object
soup = BeautifulSoup(data,"html.parser")
Definitions = soup.findAll("div", class_ = 'definition')
for result in Definitions:
categories = result.find_all("td")
KOs = categories[5].text
print()
KOs
KO_char = KOs[:]
KO_char = KO_char.strip('\n ')
KO_char = KO_char.partition('\n')[0]
#KO_char = re.sub(r'(K[0-9]+)K',r'\1 K', KO_char)
KO_char = re.sub(r'(K[0-9]+(?=K))',r'\1 ', KO_char)
KO_char = re.sub("\s+", ";", KO_char.strip())
KO_char = re.sub(r"\([^\(\)]+\)", lambda x: x.group(0).replace(";", ","), KO_char)
KO_char = re.sub(r'\((.*?)\)',r'\1', KO_char)
KO_char = re.sub(r"\([^\(\)]+\)", lambda x: x.group(0).replace(";", ","), KO_char)
KO_char = re.sub(r'\((.*?)\)',r'\1', KO_char)
KO_char = re.sub("\(", "", KO_char.strip())
KO_char = re.sub("\)", "", KO_char.strip())
KO_char = re.sub("\+", ",", KO_char.strip())
KO_char = re.sub("\-", ",", KO_char.strip())
KO_char = re.sub("-", '', KO_char)
KO_char = KO_char.split(";")
#print to df
df = pd.DataFrame({'KO_ID': KO_char})
#add dummy columns for the order and add column for module ID
df["Order"] = np.arange(df.shape[0])+1
df["Module"] = module_id
#split
df['KO_ID'] = df['KO_ID'].str.split(',')
df = df.explode('KO_ID')
#drop empty columns
df= df[df.KO_ID != '']
df
#print
#df.to_csv(module_id+'.txt', sep ='\t', index = False)
module_id = "M00133"
url = "https://www.genome.jp/module/" + module_id
#use get to download the contents of the webpage in text format and store in a variable called data:
data = requests.get(url).text
#create a beautiful soup object
soup = BeautifulSoup(data,"html.parser")
Definitions = soup.findAll("div", class_ = 'definition')
for result in Definitions:
categories = result.find_all("td")
KOs = categories[5].text
print()
KO_char = KOs[:]
KO_char = KO_char.strip('\n ')
KO_char = KO_char.partition('\n')[0]
#KO_char = re.sub(r'(K[0-9]+)K',r'\1 K', KO_char)
KO_char = re.sub(r'(K[0-9]+(?=K))',r'\1 ', KO_char)
KO_char = re.sub("\s+", ";", KO_char.strip())
KO_char = re.sub(r"\([^\(\)]+\)", lambda x: x.group(0).replace(";", ","), KO_char)
KO_char = re.sub(r'\((.*?)\)',r'\1', KO_char)
KO_char = re.sub(r"\([^\(\)]+\)", lambda x: x.group(0).replace(";", ","), KO_char)
KO_char = re.sub(r'\((.*?)\)',r'\1', KO_char)
KO_char = re.sub("\(", "", KO_char.strip())
KO_char = re.sub("\)", "", KO_char.strip())
KO_char = re.sub("\+", ",", KO_char.strip())
KO_char = re.sub("\-", ",", KO_char.strip())
KO_char = re.sub("-", '', KO_char)
KO_char = KO_char.split(";")
#print to df
df = pd.DataFrame({'KO_ID': KO_char})
#add dummy columns for the order and add column for module ID
df["Order"] = np.arange(df.shape[0])+1
df["Module"] = module_id
#split
df['KO_ID'] = df['KO_ID'].str.split(',')
df = df.explode('KO_ID')
#drop empty columns
df= df[df.KO_ID != '']
df
#print
#df.to_csv(module_id+'.txt', sep ='\t', index = False)
#import libs for the webscraping and downloading the web page
from bs4 import BeautifulSoup
import requests
import re
import pandas as pd
import numpy as np
kegg_id = "K00657"
url = "https://www.genome.jp/dbget-bin/www_bget?" + kegg_id
#use get to download the contents of the webpage in text format and store in a variable called data:
data = requests.get(url).text
#create a beautiful soup object
soup = BeautifulSoup(data,"html.parser")
for link in soup.findAll('a'):
print(link['href'])
for link in soup.findAll('a'):
print(link['href'])
soup
soup = BeautifulSoup(source_code.content, 'lxml')
soup = BeautifulSoup(data.content, 'lxml')
data = requests.get(url)
soup = BeautifulSoup(data.content, 'lxml')
soup
links = []
for link in soup.find_all('a'):
links.append(str(link))
links
Definitions = soup.findAll("td", class_ = 'vtop pd0')
Definitions
Definitions = soup.findAll("td", class_ = 'Other DBs')
Definitions
url = "https://www.genome.jp/dbget-bin/www_bget?" + kegg_id
#use get to download the contents of the webpage in text format and store in a variable called data:
data = requests.get(url).text
#create a beautiful soup object
soup = BeautifulSoup(data,"html.parser")
Definitions = soup.findAll("tr", class_ = 'th41 deft tal vtop')
Definitions
Definitions = soup.findAll("tr", class_ = 'nowrap')
Definitions
soup
Definitions = soup.findAll("td", class_ = 'w1')
Definitions
url_list = soup.find_all('a', href=True
url_list
url_list = soup.find_all('a', href=True)
url_list
pattern = re.compile(r"http://www.ncbi/.*?/.*?")
links = soup.find_all("a", href=pattern)
links
pattern = re.compile(r"http://www.ncbi.*?/")
links
pattern
pattern = re.compile(r"http://www.ncbi.*?")
links = soup.find_all("a", href=pattern)
links
pattern = re.compile(r"http://www.ncbi?")
links = soup.find_all("a", href=pattern)
links
View(url_list)
pattern = re.compile(r"https://www.ncbi?")
links = soup.find_all("a", href=pattern)
links
pattern = re.compile(r"https://www.ncbi?")
links = soup.find_all("a", href=pattern).text
links
type(links)
pattern = re.compile(r"https://www.ncbi?COG?")
links = soup.find_all("a", href=pattern)
links
pattern = re.compile(r"https://www.ncbi?*COG?*")
pattern = re.compile(r"https://www.ncbi.*?COG.*?")
links = soup.find_all("a", href=pattern)
links
links.text
url = "https://www.genome.jp/dbget-bin/www_bget?" + kegg_id
#use get to download the contents of the webpage in text format and store in a variable called data:
data = requests.get(url).text
#create a beautiful soup object
soup = BeautifulSoup(data,"html.parser")
pattern = re.compile(r"https://www.ncbi.*?COG.*?")
links = soup.find_all("a", href=pattern)
links
type(links)
links[0]
links[0].text
pattern = re.compile(r"https://www.ncbi.*?COG.*?")
COG_ID = soup.find_all("a", href=pattern)[0].text
COG_ID
COG_to_Kegg_dic = {kegg_id:cog_id}
#make a dic
COG_to_Kegg_dic = {kegg_id:COG_ID}
COG_to_Kegg_dic
pd.DataFrame(COG_to_Kegg_dic)
#make a dic
COG_to_Kegg_dic = {kegg_id:COG_ID}
#to df
pd.DataFrame(COG_to_Kegg_dic.items(), columns=['KEGG_id', 'COG_id'])
kegg_id = "K12447"
url = "https://www.genome.jp/dbget-bin/www_bget?" + kegg_id
#use get to download the contents of the webpage in text format and store in a variable called data:
data = requests.get(url).text
#create a beautiful soup object
soup = BeautifulSoup(data,"html.parser")
pattern = re.compile(r"https://www.ncbi.*?COG.*?")
COG_ID = soup.find_all("a", href=pattern)[0].text
COG_ID
#make a dic
COG_to_Kegg_dic = {kegg_id:COG_ID}
#to df
COG_to_Kegg_df = pd.DataFrame(COG_to_Kegg_dic.items(), columns=['KEGG_id', 'COG_id'])
COG_to_Kegg_df
url = "https://www.genome.jp/dbget-bin/www_bget?" + kegg_id
#use get to download the contents of the webpage in text format and store in a variable called data:
data = requests.get(url).text
#create a beautiful soup object
soup = BeautifulSoup(data,"html.parser")
